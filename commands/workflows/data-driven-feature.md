# Data-Driven Feature Workflow

Implement ML/data science features with complete data pipeline integration.

## Usage
/data-driven-feature [feature description] [data requirements]

## Example
/data-driven-feature recommendation system using user behavior data

## Workflow Steps

1. **Data Analysis Agent**
   - Analyze data requirements
   - Identify data sources
   - Plan data collection strategy
   - Design data schema
   - Plan data storage

2. **Data Pipeline Agent**
   - Design data ingestion pipeline
   - Create ETL processes
   - Set up data transformation
   - Design data validation
   - Plan data quality checks

3. **ML/Model Agent**
   - Design ML model architecture
   - Create feature engineering
   - Implement model training
   - Set up model evaluation
   - Plan model deployment

4. **API Integration Agent**
   - Create prediction API
   - Implement model serving
   - Add caching layer
   - Handle batch predictions
   - Add real-time predictions

5. **Monitoring Agent**
   - Set up model monitoring
   - Create data quality monitoring
   - Add performance tracking
   - Set up alerts
   - Plan model retraining

## Output
- Data pipeline implementation
- ML model code
- API endpoints
- Monitoring setup
- Documentation
- Test suite

## Variables
$ARGUMENTS - Feature description and data requirements

## Supported Frameworks
- Python: scikit-learn, TensorFlow, PyTorch
- Data Processing: Apache Spark, Pandas, Dask
- ML Ops: MLflow, Kubeflow, SageMaker
- Data Pipelines: Airflow, Prefect, Dagster

## Data Pipeline Components
- Data ingestion
- Data transformation
- Feature engineering
- Model training
- Model serving
- Model monitoring

